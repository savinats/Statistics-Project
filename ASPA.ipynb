{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **NAIVE BAYS CLASSIFIEER FOR FAKE NEWS RECOGNITON**"
      ],
      "metadata": {
        "id": "QtEhm_o48wVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Contributors:*\n",
        "- Savina Tsichli\n",
        "- Marco Foster"
      ],
      "metadata": {
        "id": "32FLgVuwABzs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Fake news are defined by the New York Times as ”a made-up story with an intention to deceive”, with\n",
        "the intent to confuse or deceive people. They are everywhere in our daily life and they come especially\n",
        "from social media platforms and applications in the online world. Being able to distinguish fake\n",
        "contents form real news is today one of the most serious challenges facing the news industry. Naive\n",
        "Bayes classifiers are powerful algorithms that are used for text data analysis and are connected to\n",
        "classification tasks of text in multiple classes. The goal of the project is to implement a Multinomial\n",
        "Naive Bayes classifier in R and test its performances in the classification of social media posts.*"
      ],
      "metadata": {
        "id": "IFJ1Ol7WXen2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Introduction"
      ],
      "metadata": {
        "id": "FgVKthBKXgYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset 1: Kaggle Multiclass Fake News Dataset\n",
        "The Kaggle dataset contains 6 possible labels:\n",
        "- True (5)\n",
        "- Not-Known (4)\n",
        "- Mostly-True (3)\n",
        "- Half-True (2)\n",
        "- False (1)\n",
        "- Barely-True (0)\n",
        "\n",
        "## Dataset 2: Binary Dataset\n",
        "This dataset contains two labels:\n",
        "- Reliable (0)\n",
        "- Unreliable (1)\n",
        "\n",
        "## Preprocessing\n",
        "\n",
        "To prepare the data for classification, we employ the following steps:\n",
        "\n",
        "### Tokenization\n",
        "We split the text into individual words or tokens. Tokenization simplifies analysis by focusing on each word as a separate unit.\n",
        "\n",
        "### Stopword Removal\n",
        "Stopwords are common words like \"and\" or \"the\" that add little semantic value to the text. Removing them allows the model to focus on more important words.\n",
        "\n",
        "### Normalization\n",
        "Normalization reduces words to their base form, making words like \"running\" and \"run\" equivalent. This helps reduce the feature space by treating variations of the same word as one.\n"
      ],
      "metadata": {
        "id": "bcfk5ZQr_O0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Objective"
      ],
      "metadata": {
        "id": "pPAYKtnbYjDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this project is to classify news articles into multiple categories (ranging from \"False\" to \"True\") using a **Naive Bayes classifier**. By analyzing the text in news articles, we aim to detect their factuality based on predefined labels. The dataset is split into training, validation, and test sets, and we follow standard text preprocessing techniques, including tokenization, stopword removal, and normalization.\n"
      ],
      "metadata": {
        "id": "EiA_5_LYXBY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Naive Bayes Classifier"
      ],
      "metadata": {
        "id": "sL2Y8UQxYpV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **Naive Bayes classifier** is a probabilistic machine learning model used for classification tasks. It is based on Bayes' Theorem, assuming independence between features. Despite this \"naive\" assumption, it performs well in real-world applications, especially for text classification, such as spam detection or sentiment analysis. The algorithm computes the probability of each class given a feature and selects the class with the highest likelihood. It is efficient, easy to implement, and works well with large datasets."
      ],
      "metadata": {
        "id": "td1nWzzbBgtn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Code"
      ],
      "metadata": {
        "id": "-ICZgHHsZHg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load packages\n",
        "\n",
        "package <- c(\"tokenizers\", \"tidytext\", \"dplyr\", \"tm\", \"SnowballC\", \"e1071\", \"caret\", \"readr\")\n",
        "install.packages(package)\n",
        "install.packages(\"data.table\")\n",
        "\n",
        "library(tokenizers)\n",
        "library(tidytext)\n",
        "library(dplyr)\n",
        "library(tm)\n",
        "library(SnowballC)\n",
        "library(e1071)\n",
        "library(caret)\n",
        "library(readr)\n",
        "library(data.table)"
      ],
      "metadata": {
        "id": "qczqEs4JD2VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Training (Binary Classification)**\n",
        "\n",
        "After preparing the second dataset, a Naive Bayes model was trained using the training data.\n",
        "The data was split into training (85%) and validation (15%) sets."
      ],
      "metadata": {
        "id": "zfl3KQMo0aSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df <- read_csv(\"train.csv\")\n",
        "test <- read_csv(\"test.csv\")\n",
        "\n",
        "#Split Data into Training and Validation Sets\n",
        "index <- nrow(df) * 0.85\n",
        "train <- df[1:index, ]\n",
        "val <- df[(index + 1):nrow(df), ]\n",
        "\n",
        "print(nrow(test))\n",
        "print(nrow(df))\n",
        "print(nrow(train))\n",
        "print(nrow(val))\n",
        "\n",
        "#Extract the Labels and Text columns\n",
        "y <- train$Labels\n",
        "Text <- train[[\"Text\"]]\n",
        "\n",
        "#Tokenize the text and store tokens in a list\n",
        "tokens_list <- lapply(Text, tokenize_words)\n",
        "#print(head(tokens_list))\n",
        "\n",
        "#Extract the Labels and Text columns\n",
        "train_y <- factor(y, levels = c(0, 1, 2, 3, 4, 5))\n",
        "val_y <- factor(val$Labels, levels = c(0, 1, 2, 3, 4, 5))\n",
        "\n",
        "TrainText <- train[[\"Text\"]]\n",
        "ValText <- val[[\"Text\"]]\n",
        "TestText <- test[[\"Text\"]]\n",
        "\n",
        "# Tokenize the text and store tokens in a list\n",
        "tokens_train <- lapply(TrainText, tokenize_words)\n",
        "tokens_train <- lapply(tokens_train, function(x) setdiff(x, stopwords(\"en\")))\n",
        "\n",
        "tokens_val <- lapply(ValText, tokenize_words)\n",
        "tokens_val <- lapply(tokens_val, function(x) setdiff(x, stopwords(\"en\")))\n",
        "\n",
        "tokens_test <- lapply(TestText, tokenize_words)\n",
        "tokens_test <- lapply(tokens_test, function(x) setdiff(x, stopwords(\"en\")))\n",
        "\n",
        "#print(head(tokens_train))\n",
        "#print(head(tokens_val))\n",
        "#print(head(tokens_test))\n",
        "\n",
        "#Create a text corpus for each set\n",
        "#Creates text corpora from the tokenized lists for each dataset using the tm package.\n",
        "trainCorpus <- Corpus(VectorSource(tokens_train))\n",
        "valCorpus <- Corpus(VectorSource(tokens_val))\n",
        "testCorpus <- Corpus(VectorSource(tokens_test))\n",
        "\n",
        "#Create document-term matrices\n",
        "train_dtm <- DocumentTermMatrix(trainCorpus)\n",
        "train_dtm <- removeSparseTerms(train_dtm, 0.95) #Removes sparse terms from the training DTM (terms appearing in less than 5% of the documents).\n",
        "val_dtm <- DocumentTermMatrix(valCorpus, control = list(dictionary = Terms(train_dtm)))\n",
        "test_dtm <- DocumentTermMatrix(testCorpus, control = list(dictionary = Terms(train_dtm)))\n",
        "\n",
        "## Reduce the number of features in the DTMs\n",
        "#train_dtm <- removeSparseTerms(train_dtm, 0.99) # Keep terms that appear in at least 1% of documents\n",
        "#val_dtm <- removeSparseTerms(val_dtm, 0.99)\n",
        "#test_dtm <- removeSparseTerms(test_dtm, 0.99)"
      ],
      "metadata": {
        "id": "Yc0k26Q4ZPEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare Data for Modeling**"
      ],
      "metadata": {
        "id": "2SturI6V3JVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert DTMs to Matrices for easier manipulation\n",
        "train_matrix <- as.matrix(train_dtm)\n",
        "val_matrix <- as.matrix(val_dtm)\n",
        "test_matrix <- as.matrix(test_dtm)\n",
        "\n",
        "#Matrix columns to factors for categorization\n",
        "for (cols in colnames(train_matrix)) {\n",
        "  train_matrix[, cols] <- factor(train_matrix[, cols])\n",
        "}\n",
        "\n",
        "for (cols in colnames(val_matrix)) {\n",
        "  val_matrix[, cols] <- factor(val_matrix[, cols])\n",
        "}\n",
        "\n",
        "for (cols in colnames(test_matrix)) {\n",
        "  test_matrix[, cols] <- factor(test_matrix[, cols])\n",
        "}\n",
        "\n",
        "#Ensure Labels column is a factor with the correct levels\n",
        "train$Labels <- factor(train$Labels, levels = c(0, 1, 2, 3, 4, 5))\n",
        "val$Labels <- factor(val$Labels, levels = c(0, 1, 2, 3, 4, 5))\n",
        "\n",
        "#Combine Labels and DTM matrix in a data frame\n",
        "#The labels are combined with the training and validation matrices to prepare for model training.\n",
        "train_matrix <- data.frame(Labels = train$Labels, train_matrix)\n",
        "val_matrix <- data.frame(Labels = val$Labels, val_matrix)"
      ],
      "metadata": {
        "id": "LvSF1sCw3ITN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Training**"
      ],
      "metadata": {
        "id": "_q-INQVM4i6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the Multinomial Naive Bayes classifier\n",
        "model <- naiveBayes(Labels ~ ., data = train_matrix)\n",
        "\n",
        "#Predict on validation set\n",
        "valPred <- predict(model, newdata = val_matrix)\n",
        "\n",
        "#Convert predictions and true labels to factors with the same levels\n",
        "all_levels <- c(0, 1, 2, 3, 4, 5)\n",
        "valPred <- factor(valPred, levels = all_levels)\n",
        "val_matrix$Labels <- factor(val_matrix$Labels, levels = all_levels)\n",
        "\n",
        "#Evaluate the model\n",
        "cm <- confusionMatrix(valPred, val_matrix$Labels)\n",
        "print(cm)\n",
        "\n",
        "#Predict on test set\n",
        "#testPred <- predict(model, newdata = test_final)\n",
        "\n",
        "#Ensure test predictions have the same factor levels (optional, depending on use case)\n",
        "#testPred <- factor(testPred, levels = all_levels)\n",
        "\n",
        "#Output test predictions\n",
        "#print(testPred)"
      ],
      "metadata": {
        "id": "kBsEvWMNa_6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final dataset**\n"
      ],
      "metadata": {
        "id": "-9KIQ_X21NXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df <- read_csv(\"train2.csv\")\n",
        "test <- read_csv(\"test2.csv\")\n",
        "\n",
        "index <- nrow(df) * 0.85\n",
        "train <- df[1:index, ]\n",
        "val <- df[(index + 1):nrow(df), ]\n",
        "\n",
        "print(nrow(test))\n",
        "print(nrow(df))\n",
        "print(nrow(train))\n",
        "print(nrow(val))\n",
        "\n",
        "#Extract the Labels and Text columns\n",
        "y <- train$label\n",
        "Text <- train[[\"text\"]]\n",
        "\n",
        "#Tokenize the text and store tokens in a list\n",
        "tokens_list <- lapply(Text, tokenize_words)\n",
        "#print(head(tokens_list))\n",
        "\n",
        "#Extract the Labels and Text columns\n",
        "train_y <- factor(y, levels = c(0, 1))\n",
        "val_y <- factor(val$label, levels = c(0, 1))\n",
        "\n",
        "TrainText <- train[[\"text\"]]\n",
        "ValText <- val[[\"text\"]]\n",
        "TestText <- test[[\"text\"]]\n",
        "\n",
        "#Tokenize the text and store tokens in a list\n",
        "tokens_train <- lapply(TrainText, tokenize_words)\n",
        "tokens_train <- lapply(tokens_train, function(x) setdiff(x, stopwords(\"en\")))\n",
        "\n",
        "tokens_val <- lapply(ValText, tokenize_words)\n",
        "tokens_val <- lapply(tokens_val, function(x) setdiff(x, stopwords(\"en\")))\n",
        "\n",
        "tokens_test <- lapply(TestText, tokenize_words)\n",
        "tokens_test <- lapply(tokens_test, function(x) setdiff(x, stopwords(\"en\")))\n",
        "\n",
        "###\n",
        "#print(head(tokens_train))\n",
        "#print(head(tokens_val))\n",
        "#print(head(tokens_test))\n",
        "\n",
        "#Create a text corpus for each set\n",
        "trainCorpus <- Corpus(VectorSource(tokens_train))\n",
        "valCorpus <- Corpus(VectorSource(tokens_val))\n",
        "testCorpus <- Corpus(VectorSource(tokens_test))\n",
        "\n",
        "#Create document-term matrices\n",
        "train_dtm <- DocumentTermMatrix(trainCorpus)\n",
        "train_dtm <- removeSparseTerms(train_dtm, 0.95)\n",
        "val_dtm <- DocumentTermMatrix(valCorpus, control = list(dictionary = Terms(train_dtm)))\n",
        "test_dtm <- DocumentTermMatrix(testCorpus, control = list(dictionary = Terms(train_dtm)))\n",
        "\n",
        "# Reduce the number of features in your DTMs\n",
        "# Try removing sparse terms\n",
        "#train_dtm <- removeSparseTerms(train_dtm, 0.99) # Keep terms that appear in at least 1% of documents\n",
        "#val_dtm <- removeSparseTerms(val_dtm, 0.99)\n",
        "#test_dtm <- removeSparseTerms(test_dtm, 0.99)\n",
        "\n",
        "train_matrix <- as.matrix(train_dtm)\n",
        "val_matrix <- as.matrix(val_dtm)\n",
        "test_matrix <- as.matrix(test_dtm)\n",
        "\n",
        "for (cols in colnames(train_matrix)) {\n",
        "  train_matrix[, cols] <- factor(train_matrix[, cols])\n",
        "}\n",
        "\n",
        "for (cols in colnames(val_matrix)) {\n",
        "  val_matrix[, cols] <- factor(val_matrix[, cols])\n",
        "}\n",
        "\n",
        "for (cols in colnames(test_matrix)) {\n",
        "  test_matrix[, cols] <- factor(test_matrix[, cols])\n",
        "}\n",
        "\n",
        "train_matrix <- data.frame(Labels = as.factor(train$label), train_matrix)\n",
        "val_matrix <- data.frame(Labels = as.factor(val$label), val_matrix)"
      ],
      "metadata": {
        "id": "szLSA1dM1Qo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all(colnames(train_matrix) == colnames(val_matrix))  # Should return TRUE\n",
        "all(colnames(train_matrix) == colnames(test_matrix))  # Should return TRUE"
      ],
      "metadata": {
        "id": "Tu8_A7p9-EVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_matrix$Labels"
      ],
      "metadata": {
        "id": "hsq4KxbP-E9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the Multinomial Naive Bayes classifier\n",
        "model2 <- naiveBayes(Labels ~ ., data = train_matrix, laplace = 1)\n",
        "\n",
        "#Predict on validation set\n",
        "valPred2 <- predict(model2, newdata = val_matrix[,-1])\n",
        "\n",
        "#Convert predictions and true labels to factors with the same levels\n",
        "all_levels <- c(0, 1)  # Set explicitly for binary classification\n",
        "valPred2 <- factor(valPred2, levels = all_levels)\n",
        "val_matrix$Labels <- factor(val_matrix$Labels, levels = all_levels)\n",
        "\n",
        "#Evaluate the model\n",
        "cm <- confusionMatrix(valPred2, val_matrix$Labels)\n",
        "print(cm)\n",
        "\n",
        "#Predict on test set\n",
        "#testPred2 <- predict(model2, newdata = test_matrix)\n",
        "\n",
        "#Ensure test predictions have the same factor levels (optional, depending on use case)\n",
        "#testPred2 <- factor(testPred2, levels = all_levels)\n",
        "\n",
        "#Output test predictions\n",
        "#print(head(testPred2))"
      ],
      "metadata": {
        "id": "-bz0Yfl8-Gg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Transformer**\n",
        "\n",
        "Alternative Approach: Transformers for Fake News Detection"
      ],
      "metadata": {
        "id": "cMSZRHfb-YK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the Naive Bayes classifier is effective for many text classification tasks, modern approaches using transformer models have demonstrated superior performance. Transformers, such as BERT, utilize embeddings that capture contextual relationships in text, leading to better classification accuracy.\n",
        "\n",
        "In this section, we propose replacing the Naive Bayes classifier with a transformer-based model for the fake news detection task.\n"
      ],
      "metadata": {
        "id": "Ufbe5Np0-3RT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For this we will need to also install the packages:\n",
        "\n",
        "library(dplyr)\n",
        "library(torch)\n",
        "library(transformers)"
      ],
      "metadata": {
        "id": "uXxOdXn6-Wue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After loading the datasets, the labels are converted to numeric,\n",
        "# and then we extract text from both training and test datasets"
      ],
      "metadata": {
        "id": "lmU-j65WAqq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the data with a pre-trained tokenizer\n",
        "# This way we convert texts to token IDs with embeddings\n",
        "\n",
        "tokenizer <- transformers::AutoTokenizer$from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "b3gGaiaZck3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then we create a dataset that holds tokenized input and corresponding labels"
      ],
      "metadata": {
        "id": "OmJRtoApc0Z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After loading the pre-trained model,\n",
        "# we test it by running the data through it and get predictions."
      ],
      "metadata": {
        "id": "kQ3bnPBidNm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# then we evaluate the model by comparing the predictions to the actual labels,\n",
        "# and then calculate accuracy"
      ],
      "metadata": {
        "id": "rOQg_0FHeDbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary of Findings\n",
        "\n",
        "1) We implemented the Naive Bayes Classifier first on a binary dataset (0,1), and then on the Fake News multi-class dataset (0,1,2,3,4,5). We conclude that the model worked only on the multi-class dataset and not the binary. We explain this by mentioning that:\n",
        "\n",
        "- First, the labels are too similar and have a lot overlap between the  the ones that are completely true (5) to barely true (0).\n",
        "- the Naive Bayes Classifier Model assumes that features (e.g., words or phrases) are independent given the class label. In this particular example with the news articles, this assumption doesn’t hold. For example, certain phrases may often occur together in genuine articles but not in fake ones.\n",
        "- The model may not effectively capture the nuances that differentiate the two categories, especially if they share a lot of vocabulary.\n",
        "- It treats every feature independently and doesn’t consider the context or relationships between words.\n",
        "\n",
        "2) That lead us to our next step which was to use a model that takes into account the position of a token in a given phrase or sentence. The transformer is a good example of a model that uses Attention, adding embeddings to each token so as to capture semantic meanings, contextual relationships, and positional information.\n",
        "\n",
        "- By implementing a transformer-based model for fake news detection, we expect improved accuracy and reliability compared to the Naive Bayes classifier. The context-aware nature of transformers enables a deeper understanding of text, which is critical for accurately distinguishing between real and fake news.\n",
        "\n"
      ],
      "metadata": {
        "id": "heSomT0YBBTf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "elPAk1H0BCJ4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}